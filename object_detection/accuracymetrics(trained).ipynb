{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m09ECOxw0xV3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Yaswanth-B/AccessibleLLM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "1h9PxAAIviFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"arian2502/firstdataset\", split=\"train\")"
      ],
      "metadata": {
        "id": "XayEicsDYods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score torch transformers"
      ],
      "metadata": {
        "id": "RGdZmaTc1Zey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "PL83oUMY4CUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "RbPu1Tsqj5OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"arian2502/firstdataset\", split=\"train\")\n",
        "true_text = []\n",
        "\n",
        "for data_item in dataset:\n",
        "    # Assuming your dataset has a key \"caption\" for captions\n",
        "    caption = data_item[\"text\"]\n",
        "    true_text.append(caption)\n",
        "\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object_detection/generated_captions.txt\", \"r\") as model1_file:\n",
        "    model1_captions = model1_file.readlines()\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object_detection/generated_captions(trained).txt\", \"r\") as model2_file:\n",
        "    model2_captions = model2_file.readlines()\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Compute ROUGE scores for blip untrained\n",
        "scores_model1 = rouge.get_scores(model1_captions, true_text, avg=True)\n",
        "print(\"ROUGE Scores for BLIP(untrained):\")\n",
        "print(scores_model1)\n",
        "\n",
        "# Compute ROUGE scores for blip trained\n",
        "scores_model2 = rouge.get_scores(model2_captions, true_text, avg=True)\n",
        "print(\"ROUGE Scores for BLIP(trained)t:\")\n",
        "print(scores_model2)\n",
        "\n",
        "# Calculate precision, recall, and F1-score based on ROUGE scores\n",
        "precision_model1 = scores_model1[\"rouge-l\"][\"p\"]\n",
        "recall_model1 = scores_model1[\"rouge-l\"][\"r\"]\n",
        "f1_model1 = scores_model1[\"rouge-l\"][\"f\"]\n",
        "\n",
        "precision_model2 = scores_model2[\"rouge-l\"][\"p\"]\n",
        "recall_model2 = scores_model2[\"rouge-l\"][\"r\"]\n",
        "f1_model2 = scores_model2[\"rouge-l\"][\"f\"]\n",
        "\n",
        "print(\"Precision, Recall, and F1-score for BLIP(untrained):\")\n",
        "print(\"Precision:\", precision_model1)\n",
        "print(\"Recall:\", recall_model1)\n",
        "print(\"F1-score:\", f1_model1)\n",
        "\n",
        "print(\"Precision, Recall, and F1-score for BLIP(trained):\")\n",
        "print(\"Precision:\", precision_model2)\n",
        "print(\"Recall:\", recall_model2)\n",
        "print(\"F1-score:\", f1_model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrKZfonNz-9u",
        "outputId": "f80b0a44-a38a-44ae-8528-573794b18432"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores for BLIP(untrained):\n",
            "{'rouge-1': {'r': 0.38623084720771134, 'p': 0.26676010739518513, 'f': 0.3066864136993266}, 'rouge-2': {'r': 0.1298950742068691, 'p': 0.0725541792011556, 'f': 0.08984282440623012}, 'rouge-l': {'r': 0.3364418769543522, 'p': 0.23059880787255438, 'f': 0.2657411209552809}}\n",
            "ROUGE Scores for BLIP(trained)t:\n",
            "{'rouge-1': {'r': 0.7300539801249587, 'p': 0.6806757571022624, 'f': 0.6995404673734046}, 'rouge-2': {'r': 0.6493630657374209, 'p': 0.577443416297378, 'f': 0.6069535829964249}, 'rouge-l': {'r': 0.7226284310678925, 'p': 0.6730824339010327, 'f': 0.6922038904392883}}\n",
            "Precision, Recall, and F1-score for BLIP(untrained):\n",
            "Precision: 0.23059880787255438\n",
            "Recall: 0.3364418769543522\n",
            "F1-score: 0.2657411209552809\n",
            "Precision, Recall, and F1-score for BLIP(trained):\n",
            "Precision: 0.6730824339010327\n",
            "Recall: 0.7226284310678925\n",
            "F1-score: 0.6922038904392883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import bert_score\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"arian2502/firstdataset\", split=\"train\")\n",
        "true_text = []\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object_detection/generated_captions.txt\", \"r\") as file:\n",
        "    model1_text = file.read()\n",
        "\n",
        "blip_untrained = model1_text.strip().split('\\n')\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object_detection/generated_captions(trained).txt\", \"r\") as file:\n",
        "    model2_text = file.read()\n",
        "\n",
        "blip_trained = model2_text.strip().split('\\n')\n",
        "\n",
        "for data_item in dataset:\n",
        "    # Assuming your dataset has a key \"caption\" for captions\n",
        "    caption = data_item[\"text\"]\n",
        "    true_text.append(caption)\n",
        "\n",
        "# Compute BERTScore for blip(untrained) Model\n",
        "P_A, R_A, F1_A = bert_score.score(\n",
        "    blip_untrained,\n",
        "    true_text,\n",
        "    lang='en',  # Language of the captions (e.g., 'en' for English)\n",
        "    model_type='bert-base-uncased',  # BERT model type\n",
        ")\n",
        "\n",
        "# Compute BERTScore for blip(trained) Model\n",
        "P_B, R_B, F1_B = bert_score.score(\n",
        "    blip_trained,\n",
        "    true_text,\n",
        "    lang='en',  # Language of the captions (e.g., 'en' for English)\n",
        "    model_type='bert-base-uncased',  # BERT model type\n",
        ")\n",
        "\n",
        "print(\"BERTScore for BLIP(untrained) Model:\")\n",
        "print(f\"Precision: {P_A.mean():.4f}\")\n",
        "print(f\"Recall: {R_A.mean():.4f}\")\n",
        "print(f\"F1-score: {F1_A.mean():.4f}\")\n",
        "\n",
        "print(\"\\nBERTScore for BLIP(trained) Model:\")\n",
        "print(f\"Precision: {P_B.mean():.4f}\")\n",
        "print(f\"Recall: {R_B.mean():.4f}\")\n",
        "print(f\"F1-score: {F1_B.mean():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Uj0gjE07uD",
        "outputId": "385b40d6-1602-4789-a82d-c5b2dfb55abc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore for BLIP(untrained) Model:\n",
            "Precision: 0.5309\n",
            "Recall: 0.6051\n",
            "F1-score: 0.5637\n",
            "\n",
            "BERTScore for BLIP(trained) Model:\n",
            "Precision: 0.8848\n",
            "Recall: 0.8886\n",
            "F1-score: 0.8854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"arian2502/firstdataset\", split=\"train\")\n",
        "true_text = []\n",
        "\n",
        "def compute_gleu_score(generated_caption, reference_caption):\n",
        "    gen_tokens = nltk.word_tokenize(generated_caption.lower())\n",
        "    ref_tokens = nltk.word_tokenize(reference_caption.lower())\n",
        "    gleu_score = nltk.translate.gleu_score.sentence_gleu([ref_tokens], gen_tokens)\n",
        "    return gleu_score\n",
        "\n",
        "def compute_average_gleu_scores(model_captions, reference_captions):\n",
        "    total_gleu_score = 0.0\n",
        "    num_captions = len(model_captions)\n",
        "\n",
        "    for i in range(num_captions):\n",
        "        model_caption = model_captions[i]\n",
        "        ref_caption = reference_captions[i]\n",
        "        gleu_score = compute_gleu_score(model_caption, ref_caption)\n",
        "        total_gleu_score += gleu_score\n",
        "\n",
        "    average_gleu_score = total_gleu_score / num_captions\n",
        "    return average_gleu_score\n",
        "\n",
        "def read_captions_from_file(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        captions = file.read().split('\\n')\n",
        "    return [caption.strip() for caption in captions if caption.strip()]\n",
        "\n",
        "for data_item in dataset:\n",
        "    # Assuming your dataset has a key \"caption\" for captions\n",
        "    caption = data_item[\"text\"]\n",
        "    true_text.append(caption)\n",
        "\n",
        "captions_model_a = read_captions_from_file(\"/content/AccessibleLLM/object_detection/generated_captions.txt\")\n",
        "captions_model_b = read_captions_from_file(\"/content/AccessibleLLM/object_detection/generated_captions(trained).txt\")\n",
        "\n",
        "# Compute average GLEU scores for Model A and Model B\n",
        "avg_gleu_model_a = compute_average_gleu_scores(captions_model_a, true_text)\n",
        "avg_gleu_model_b = compute_average_gleu_scores(captions_model_b, true_text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average GLEU Score for BLIP(untrained):\", avg_gleu_model_a)\n",
        "print(\"Average GLEU Score for BLIP(trained):\", avg_gleu_model_b)\n",
        "\n",
        "# Determine which model performs better\n",
        "if avg_gleu_model_a > avg_gleu_model_b:\n",
        "    print(\"BLIP(untrained) performs better.\")\n",
        "elif avg_gleu_model_b > avg_gleu_model_a:\n",
        "    print(\"BLIP(trained) performs better.\")\n",
        "else:\n",
        "    print(\"Both models perform equally well.\")\n"
      ],
      "metadata": {
        "id": "a9UtGfbO1SpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5e9dce-4f5e-4c39-eb36-34653b384c9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU Score for BLIP(untrained): 0.08593777080376051\n",
            "Average GLEU Score for BLIP(trained): 0.6954368111617628\n",
            "BLIP(trained) performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"arian2502/firstdataset\", split=\"train\")\n",
        "true_text = []\n",
        "\n",
        "def read_captions_from_file(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        captions = file.read().split('\\n')\n",
        "    return [caption.strip() for caption in captions if caption.strip()]\n",
        "\n",
        "for data_item in dataset:\n",
        "    # Assuming your dataset has a key \"caption\" for captions\n",
        "    caption = data_item[\"text\"]\n",
        "    true_text.append(caption)\n",
        "\n",
        "captions_model_a = read_captions_from_file(\"/content/AccessibleLLM/object_detection/generated_captions.txt\")\n",
        "captions_model_b = read_captions_from_file(\"/content/AccessibleLLM/object_detection/generated_captions(trained).txt\")\n",
        "\n",
        "# Tokenize captions\n",
        "model1_captions = [nltk.word_tokenize(caption.lower()) for caption in captions_model_a]\n",
        "model2_captions = [nltk.word_tokenize(caption.lower()) for caption in captions_model_b]\n",
        "reference_captions = [[nltk.word_tokenize(caption.lower())] for caption in true_text]\n",
        "\n",
        "# Calculate BLEU score for each model\n",
        "bleu_score_model1 = corpus_bleu(reference_captions, model1_captions)\n",
        "bleu_score_model2 = corpus_bleu(reference_captions, model2_captions)\n",
        "\n",
        "print(\"BLEU Score for BLIP(untrained):\", bleu_score_model1)\n",
        "print(\"BLEU Score for BLIP(trained):\", bleu_score_model2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7KKHKhvwyP-",
        "outputId": "4a89fca1-f24f-4ffe-c45c-5368e2029bc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score for BLIP(untrained): 0.04215093904464002\n",
            "BLEU Score for BLIP(trained): 0.704256378969982\n"
          ]
        }
      ]
    }
  ]
}