# -*- coding: utf-8 -*-
"""usecase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRmOy0eaEc55G9SSFMIz9ZUYmbAy5bcz
"""

pip install git+https://github.com/huggingface/transformers.git
pip install pyttsx3
pip install gTTS
pip install pydub
pip install playsound

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import requests
import pyttsx3
from gtts import gTTS
from IPython.display import Audio
from pydub import AudioSegment
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = js_to_image(data)
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)

  # save image
  cv2.imwrite(filename, img)

  return filename

try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

from PIL import Image

# Open the image from the specified path
image = Image.open("/content/photo.jpg").convert('RGB')

# Resize the image
image = image.resize((596, 437))

# Display the image (assuming you have a display function like `display`)
display(image)

from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch


processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

inputs = processor(image, return_tensors="pt").to(device, torch.float32)  # Specify float32 explicitly

generated_ids = model.generate(**inputs, max_length=50, min_length=20)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
print(generated_text)

with open("my_saved_text.txt", "a") as output_file:
    # Write the generated text to the file
    text_with_newline = generated_text + "\n"

    output_file.write(text_with_newline)

    speech = gTTS(text=generated_text, lang='en')  # Change 'en' for other languages

# Save the audio to a file
speech.save("generated_text.mp3")

#Play the generated audio
from playsound import playsound
Audio("generated_text.mp3")

prompt = "this is an image of"

inputs = processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

generated_ids = model.generate(**inputs, max_new_tokens=60, min_length=30)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
print(generated_text)

with open("my_saved_text.txt", "a") as output_file:
    # Write the generated text to the file
    text_with_newline=generated_text + "\n"
    output_file.write(text_with_newline)
speech = gTTS(text=generated_text, lang='en')  # Change 'en' for other languages

# Save the audio to a file
speech.save("generated_text.mp3")

#Play the generated audio
from playsound import playsound
Audio("generated_text.mp3")

prompt = "Question: What kind of image is the person trying to display? Answer:"

inputs = processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

generated_ids = model.generate(**inputs, max_new_tokens=60, min_length=30)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

print(generated_text)
with open("my_saved_text.txt", "a") as output_file:
    # Write the generated text to the file
    text_with_newline=generated_text + "\n"
    output_file.write(text_with_newline)
speech = gTTS(text=generated_text, lang='en')  # Change 'en' for other languages

# Save the audio to a file
speech.save("generated_text.mp3")

#Play the generated audio
from playsound import playsound
Audio("generated_text.mp3")

context = [
   ("What kind of image is the person playing ?", "He is showing the image of a something on this phone"),
   ("Where is he showing it?", "In his phone")
]
question = "What for?"
template = "Question: {} Answer: {}."

prompt = " ".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + " Question: " + question + " Answer:"

print(prompt)

inputs = processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

generated_ids = model.generate(**inputs, max_new_tokens=52, min_length=30)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
print(generated_text)
with open("my_saved_text.txt", "a") as output_file:
    # Write the generated text to the file
    text_with_newline=generated_text + "\n"
    output_file.write(text_with_newline)
speech = gTTS(text=generated_text, lang='en')  # Change 'en' for other languages

# Save the audio to a file
speech.save("generated_text.mp3")

#Play the generated audio
from playsound import playsound
Audio("generated_text.mp3")

from gtts import gTTS

with open("my_saved_text.txt", "r") as text_file:
    # Read all lines into a list, handling potential file emptiness gracefully
    text_lines = text_file.readlines()
    if not text_lines:
        print("The file 'my_saved_text' is empty. No text to convert to speech.")
        exit()

language = "en"



tts_objects = []
for i, line in enumerate(text_lines):
    if not line.strip():
          continue
    gtts_object = gTTS(text=line.strip(), lang=language, slow=False)
    tts_objects.append(gtts_object)
    filename = f"my_saved_text.txt_{i+1}.wav"
    gtts_object.save(filename)
    print(f'Text "{line.strip()}" saved to audio file: {filename}')

Audio("my_saved_text.txt_1.wav")

