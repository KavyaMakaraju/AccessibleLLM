{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m09ECOxw0xV3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Yaswanth-B/AccessibleLLM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score torch transformers"
      ],
      "metadata": {
        "id": "RGdZmaTc1Zey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "PL83oUMY4CUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "c_rYBcO-8qlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "RbPu1Tsqj5OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import bert_score\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object detection/mode1d3.txt\", \"r\") as file:\n",
        "    model1_text = file.read()\n",
        "\n",
        "blip2_text = model1_text.strip().split('\\n')\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object detection/model2d3.txt\", \"r\") as file:\n",
        "    model2_text = file.read()\n",
        "\n",
        "VitGpt_text = model2_text.strip().split('\\n')\n",
        "\n",
        "with open(\"/content/ds3text.txt\", \"r\") as file:\n",
        "    references_text = file.read()\n",
        "\n",
        "true_text = references_text.strip().split('\\n')\n",
        "\n",
        "# Compute BERTScore for BLIP-2 Model\n",
        "P_A, R_A, F1_A = bert_score.score(\n",
        "    blip2_text,\n",
        "    true_text,\n",
        "    lang='en',  # Language of the captions (e.g., 'en' for English)\n",
        "    model_type='bert-base-uncased',  # BERT model type\n",
        ")\n",
        "\n",
        "# Compute BERTScore for VitGpt Model\n",
        "P_B, R_B, F1_B = bert_score.score(\n",
        "    VitGpt_text,\n",
        "    true_text,\n",
        "    lang='en',  # Language of the captions (e.g., 'en' for English)\n",
        "    model_type='bert-base-uncased',  # BERT model type\n",
        ")\n",
        "\n",
        "print(\"BERTScore for BLIP-2 Model:\")\n",
        "print(f\"Precision: {P_A.mean():.4f}\")\n",
        "print(f\"Recall: {R_A.mean():.4f}\")\n",
        "print(f\"F1-score: {F1_A.mean():.4f}\")\n",
        "\n",
        "print(\"\\nBERTScore for VitGpt Model:\")\n",
        "print(f\"Precision: {P_B.mean():.4f}\")\n",
        "print(f\"Recall: {R_B.mean():.4f}\")\n",
        "print(f\"F1-score: {F1_B.mean():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Uj0gjE07uD",
        "outputId": "13c8c3ce-303c-4cef-adb6-9279574e1e79"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore for BLIP-2 Model:\n",
            "Precision: 0.7260\n",
            "Recall: 0.7872\n",
            "F1-score: 0.7541\n",
            "\n",
            "BERTScore for VitGpt Model:\n",
            "Precision: 0.6246\n",
            "Recall: 0.6585\n",
            "F1-score: 0.6362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def compute_gleu_score(generated_caption, reference_caption):\n",
        "    gen_tokens = nltk.word_tokenize(generated_caption.lower())\n",
        "    ref_tokens = nltk.word_tokenize(reference_caption.lower())\n",
        "    gleu_score = nltk.translate.gleu_score.sentence_gleu([ref_tokens], gen_tokens)\n",
        "    return gleu_score\n",
        "\n",
        "def compute_average_gleu_scores(model_captions, reference_captions):\n",
        "    total_gleu_score = 0.0\n",
        "    num_captions = len(model_captions)\n",
        "\n",
        "    for i in range(num_captions):\n",
        "        model_caption = model_captions[i]\n",
        "        ref_caption = reference_captions[i]\n",
        "        gleu_score = compute_gleu_score(model_caption, ref_caption)\n",
        "        total_gleu_score += gleu_score\n",
        "\n",
        "    average_gleu_score = total_gleu_score / num_captions\n",
        "    return average_gleu_score\n",
        "\n",
        "def read_captions_from_file(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        captions = file.read().split('\\n')\n",
        "    return [caption.strip() for caption in captions if caption.strip()]\n",
        "\n",
        "# Read reference captions from \"truetext.txt\"\n",
        "reference_captions = read_captions_from_file(\"/content/ds3text.txt\")\n",
        "\n",
        "# Read model captions from \"model1.txt\" (for Model A) and \"model2.txt\" (for Model B)\n",
        "captions_model_a = read_captions_from_file(\"/content/AccessibleLLM/object detection/mode1d3.txt\")\n",
        "captions_model_b = read_captions_from_file(\"/content/AccessibleLLM/object detection/model2d3.txt\")\n",
        "\n",
        "# Compute average GLEU scores for Model A and Model B\n",
        "avg_gleu_model_a = compute_average_gleu_scores(captions_model_a, reference_captions)\n",
        "avg_gleu_model_b = compute_average_gleu_scores(captions_model_b, reference_captions)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average GLEU Score for BLIP-2:\", avg_gleu_model_a)\n",
        "print(\"Average GLEU Score for VItGpt:\", avg_gleu_model_b)\n",
        "\n",
        "# Determine which model performs better\n",
        "if avg_gleu_model_a > avg_gleu_model_b:\n",
        "    print(\"BLIP-2 performs better.\")\n",
        "elif avg_gleu_model_b > avg_gleu_model_a:\n",
        "    print(\"VitGpt performs better.\")\n",
        "else:\n",
        "    print(\"Both models perform equally well.\")\n"
      ],
      "metadata": {
        "id": "a9UtGfbO1SpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9ad5d5-e7bb-4438-fe99-94073dc217b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU Score for BLIP-2: 0.3281658319708012\n",
            "Average GLEU Score for VItGpt: 0.15503852724900705\n",
            "BLIP-2 performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def read_captions_from_file(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        captions = file.read().split('\\n')\n",
        "    return [caption.strip() for caption in captions if caption.strip()]\n",
        "\n",
        "# Read reference captions from \"truetext.txt\"\n",
        "reference_captions = read_captions_from_file(\"/content/ds3text.txt\")\n",
        "\n",
        "# Read model captions from \"model1.txt\" (for Model A) and \"model2.txt\" (for Model B)\n",
        "captions_model_a = read_captions_from_file(\"/content/AccessibleLLM/object detection/mode1d3.txt\")\n",
        "captions_model_b = read_captions_from_file(\"/content/AccessibleLLM/object detection/model2d3.txt\")\n",
        "\n",
        "# Tokenize captions\n",
        "model1_captions = [nltk.word_tokenize(caption.lower()) for caption in captions_model_a]\n",
        "model2_captions = [nltk.word_tokenize(caption.lower()) for caption in captions_model_b]\n",
        "reference_captions = [[nltk.word_tokenize(caption.lower())] for caption in reference_captions]\n",
        "\n",
        "# Calculate BLEU score for each model\n",
        "bleu_score_model1 = corpus_bleu(reference_captions, model1_captions)\n",
        "bleu_score_model2 = corpus_bleu(reference_captions, model2_captions)\n",
        "\n",
        "print(\"BLEU Score for BLIP-2:\", bleu_score_model1)\n",
        "print(\"BLEU Score for VitGpt:\", bleu_score_model2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7KKHKhvwyP-",
        "outputId": "20f3b0b1-b468-4a27-dbb6-06a3b6803d63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score for BLIP-2: 0.2853194240578867\n",
            "BLEU Score for VitGpt: 0.09539884316244567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Load reference captions and generated captions\n",
        "with open(\"/content/ds3text.txt\", \"r\") as ref_file:\n",
        "    reference_captions = ref_file.readlines()\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object detection/mode1d3.txt\", \"r\") as model1_file:\n",
        "    model1_captions = model1_file.readlines()\n",
        "\n",
        "with open(\"/content/AccessibleLLM/object detection/model2d3.txt\", \"r\") as model2_file:\n",
        "    model2_captions = model2_file.readlines()\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Compute ROUGE scores for model 1\n",
        "scores_model1 = rouge.get_scores(model1_captions, reference_captions, avg=True)\n",
        "print(\"ROUGE Scores for BLIP-2:\")\n",
        "print(scores_model1)\n",
        "\n",
        "# Compute ROUGE scores for model 2\n",
        "scores_model2 = rouge.get_scores(model2_captions, reference_captions, avg=True)\n",
        "print(\"ROUGE Scores for VitGpt:\")\n",
        "print(scores_model2)\n",
        "\n",
        "# Calculate precision, recall, and F1-score based on ROUGE scores\n",
        "precision_model1 = scores_model1[\"rouge-l\"][\"p\"]\n",
        "recall_model1 = scores_model1[\"rouge-l\"][\"r\"]\n",
        "f1_model1 = scores_model1[\"rouge-l\"][\"f\"]\n",
        "\n",
        "precision_model2 = scores_model2[\"rouge-l\"][\"p\"]\n",
        "recall_model2 = scores_model2[\"rouge-l\"][\"r\"]\n",
        "f1_model2 = scores_model2[\"rouge-l\"][\"f\"]\n",
        "\n",
        "print(\"Precision, Recall, and F1-score for BLIP-2:\")\n",
        "print(\"Precision:\", precision_model1)\n",
        "print(\"Recall:\", recall_model1)\n",
        "print(\"F1-score:\", f1_model1)\n",
        "\n",
        "print(\"Precision, Recall, and F1-score for VitGpt:\")\n",
        "print(\"Precision:\", precision_model2)\n",
        "print(\"Recall:\", recall_model2)\n",
        "print(\"F1-score:\", f1_model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrKZfonNz-9u",
        "outputId": "9bc04303-c409-44b1-e7ae-789683c1ffbe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores for BLIP-2:\n",
            "{'rouge-1': {'r': 0.7057404605198723, 'p': 0.5711694838529204, 'f': 0.6259112389882882}, 'rouge-2': {'r': 0.46176681935195857, 'p': 0.35838442697653206, 'f': 0.39949114886886683}, 'rouge-l': {'r': 0.6315150650003591, 'p': 0.5119797835463471, 'f': 0.560635915103244}}\n",
            "ROUGE Scores for VitGpt:\n",
            "{'rouge-1': {'r': 0.44236701370524906, 'p': 0.4042870038458274, 'f': 0.41503899617383466}, 'rouge-2': {'r': 0.20881120625160865, 'p': 0.16989874025183618, 'f': 0.18336638637340974}, 'rouge-l': {'r': 0.39817976304741015, 'p': 0.3620736453089395, 'f': 0.37246338708799215}}\n",
            "Precision, Recall, and F1-score for BLIP-2:\n",
            "Precision: 0.5119797835463471\n",
            "Recall: 0.6315150650003591\n",
            "F1-score: 0.560635915103244\n",
            "Precision, Recall, and F1-score for VitGpt:\n",
            "Precision: 0.3620736453089395\n",
            "Recall: 0.39817976304741015\n",
            "F1-score: 0.37246338708799215\n"
          ]
        }
      ]
    }
  ]
}